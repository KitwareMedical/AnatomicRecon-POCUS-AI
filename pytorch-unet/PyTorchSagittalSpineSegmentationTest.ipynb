{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "from random import sample\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import girder_client\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from pytorch_unet import UNet\n",
    "import evaluation_metrics\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_notebook_name = \"PyTorchSagittalSpineSegmentationTest\"\n",
    "\n",
    "# Update this folder name for your computer\n",
    "\n",
    "train_timestamp = \"2021-07-28_23-42-59\"\n",
    "local_data_folder = r\"/home/nick/dev/SaggitalSpineSegmentation_Data\"\n",
    "\n",
    "overwrite_existing_data_files = False\n",
    "\n",
    "# All results and output will be archived with this timestamp\n",
    "\n",
    "# Evaluation parameters\n",
    "\n",
    "acceptable_margin_mm = 1.0\n",
    "mm_per_pixel = 1.0\n",
    "\n",
    "roc_thresholds = [0.9, 0.8, 0.7, 0.65, 0.6, 0.55, 0.5, 0.45, 0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1,\n",
    "                  0.08, 0.06, 0.04, 0.02, 0.01,\n",
    "                  0.008, 0.006, 0.004, 0.002, 0.001]\n",
    "\n",
    "limit_rounds = 0\n",
    "\n",
    "# Uncomment for faster debugging\n",
    "# roc_thresholds = [0.8, 0.6, 0.4, 0.2, 0.1, 0.01]\n",
    "# limit_rounds = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use cuda GPU if available\n",
    "\n",
    "# device_name = \" \"\n",
    "# if torch.cuda.is_available():\n",
    "#     device_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "# else:\n",
    "#     device_name = 'CPU'\n",
    "    \n",
    "# print('Using device:', device_name)\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_ultrasound_filename = \"ultrasound-test.npy\"\n",
    "testing_ultrasound_id = \"5daa85edd9e6a3be02d012e7\"\n",
    "testing_segmentation_filename = \"segmentation-test.npy\"\n",
    "testing_segmentation_id = \"5daa85e7d9e6a3be02d012e4\"\n",
    "\n",
    "# Default subfolders of main project data folder\n",
    "\n",
    "data_arrays_folder      = \"DataArrays\"\n",
    "notebooks_save_folder   = \"SavedNotebooks\"\n",
    "models_folder           = \"SavedModels\"\n",
    "results_save_folder     = \"SavedResults\"\n",
    "test_predictions_folder = \"PredictionsTest\"\n",
    "\n",
    "data_arrays_fullpath      = os.path.join(local_data_folder, data_arrays_folder)\n",
    "notebooks_save_fullpath   = os.path.join(local_data_folder, notebooks_save_folder)\n",
    "models_fullpath           = os.path.join(local_data_folder, models_folder)\n",
    "results_save_fullpath     = os.path.join(local_data_folder, results_save_folder)\n",
    "test_predictions_fullpath = os.path.join(local_data_folder, test_predictions_folder)\n",
    "\n",
    "if not os.path.exists(data_arrays_fullpath):\n",
    "    os.makedirs(data_arrays_fullpath)\n",
    "    print(\"Created folder: {}\".format(data_arrays_fullpath))\n",
    "\n",
    "if not os.path.exists(notebooks_save_fullpath):\n",
    "    os.makedirs(notebooks_save_fullpath)\n",
    "    print(\"Created folder: {}\".format(notebooks_save_fullpath))\n",
    "\n",
    "if not os.path.exists(models_fullpath):\n",
    "    raise FileNotFoundError(models_fullpath)\n",
    "\n",
    "if not os.path.exists(results_save_fullpath):\n",
    "    os.makedirs(results_save_fullpath)\n",
    "    print(\"Created folder: {}\".format(results_save_fullpath))\n",
    "\n",
    "if not os.path.exists(test_predictions_fullpath):\n",
    "    os.makedirs(test_predictions_fullpath)\n",
    "    print(\"Created folder: {}\".format(test_predictions_fullpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading testing files ...\n",
      "\n",
      "Total download time: 0:00:00.000903\n"
     ]
    }
   ],
   "source": [
    "# Download data from Girder\n",
    "\n",
    "time_download_start = datetime.datetime.now()\n",
    "\n",
    "print(\"Downloading testing files ...\")\n",
    "\n",
    "# Preparing progress bar\n",
    "\n",
    "girder_api_url = \"https://pocus.cs.queensu.ca/api/v1\"\n",
    "gclient = girder_client.GirderClient(apiUrl=girder_api_url)\n",
    "\n",
    "test_ultrasound_fullname = os.path.join(data_arrays_fullpath, testing_ultrasound_filename)\n",
    "if not os.path.exists(test_ultrasound_fullname):\n",
    "    print(\"Downloading {}...\".format(test_ultrasound_fullname))\n",
    "    gclient.downloadFile(testing_ultrasound_id, test_ultrasound_fullname)\n",
    "\n",
    "test_segmentation_fullname = os.path.join(data_arrays_fullpath, testing_segmentation_filename)\n",
    "if not os.path.exists(test_segmentation_fullname) or overwrite_existing_data_files:\n",
    "    print(\"Downloading {}...\".format(test_segmentation_fullname))\n",
    "    gclient.downloadFile(testing_segmentation_id, test_segmentation_fullname)\n",
    "    \n",
    "time_download_stop = datetime.datetime.now()\n",
    "print(\"\\nTotal download time: {}\".format(time_download_stop - time_download_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time to load from files: 0:00:00.035368\n"
     ]
    }
   ],
   "source": [
    "# Load numpy data\n",
    "# convert ultrasound array to torch.tensor channels first\n",
    "# Keep segmentation in numpy/TF format\n",
    "\n",
    "time_start = datetime.datetime.now()\n",
    "\n",
    "test_ultrasound_fullname = os.path.join(data_arrays_fullpath, testing_ultrasound_filename)\n",
    "test_ultrasound_array = np.load(test_ultrasound_fullname)\n",
    "test_ultrasound_array = torch.Tensor(test_ultrasound_array).permute(0,3,1,2).float()\n",
    "\n",
    "test_segmentation_fullname = os.path.join(data_arrays_fullpath, testing_segmentation_filename)\n",
    "test_segmentation_array = np.load(test_segmentation_fullname)\n",
    "\n",
    "time_stop = datetime.datetime.now()\n",
    "print(\"\\nTotal time to load from files: {}\".format(time_stop - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1892, 1, 128, 128]) (1892, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "print(test_ultrasound_array.size(), test_segmentation_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for models by /home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/*2021-07-28_23-42-59*\n",
      "Found 8 models\n"
     ]
    }
   ],
   "source": [
    "filename_pattern = \"*\" + train_timestamp + \"*\"\n",
    "search_string = os.path.join(models_fullpath, filename_pattern)\n",
    "print(\"Searching for models by {}\".format(search_string))\n",
    "model_file_list = glob.glob(search_string)\n",
    "\n",
    "num_models = len(model_file_list)\n",
    "print(\"Found {} models\".format(num_models))\n",
    "\n",
    "if limit_rounds > 0:\n",
    "    num_rounds = min(num_models, limit_rounds)\n",
    "else:\n",
    "    num_rounds = num_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wcce = torch.nn.CrossEntropyLoss(weight=torch.tensor([0.05, 0.95]).float())\n",
    "# wcce = weighted_categorical_crossentropy([0.05, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-3_2021-07-28_23-42-59.msd', '/home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-4_2021-07-28_23-42-59.msd', '/home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-7_2021-07-28_23-42-59.msd', '/home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-0_2021-07-28_23-42-59.msd', '/home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-5_2021-07-28_23-42-59.msd', '/home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-1_2021-07-28_23-42-59.msd', '/home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-2_2021-07-28_23-42-59.msd', '/home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-6_2021-07-28_23-42-59.msd']\n"
     ]
    }
   ],
   "source": [
    "print(model_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using timestemp to find files: 2021-07-28_23-42-59\n",
      "Saving test predictions in:    /home/nick/dev/SaggitalSpineSegmentation_Data/PredictionsTest\n",
      "Testing model: /home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-3_2021-07-28_23-42-59.msd\n",
      "(1892, 128, 128, 2)\n",
      "Testing round time: 0:00:42.933560\n",
      "Testing model: /home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-4_2021-07-28_23-42-59.msd\n",
      "(1892, 128, 128, 2)\n",
      "Testing round time: 0:00:42.899110\n",
      "Testing model: /home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-7_2021-07-28_23-42-59.msd\n",
      "(1892, 128, 128, 2)\n",
      "Testing round time: 0:00:43.210762\n",
      "Testing model: /home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-0_2021-07-28_23-42-59.msd\n",
      "(1892, 128, 128, 2)\n",
      "Testing round time: 0:00:43.913363\n",
      "Testing model: /home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-5_2021-07-28_23-42-59.msd\n",
      "(1892, 128, 128, 2)\n",
      "Testing round time: 0:00:43.063570\n",
      "Testing model: /home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-1_2021-07-28_23-42-59.msd\n",
      "(1892, 128, 128, 2)\n",
      "Testing round time: 0:00:43.498192\n",
      "Testing model: /home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-2_2021-07-28_23-42-59.msd\n",
      "(1892, 128, 128, 2)\n",
      "Testing round time: 0:00:43.749737\n",
      "Testing model: /home/nick/dev/SaggitalSpineSegmentation_Data/SavedModels/PyTorchSagittalSpineSegmentationStudy_model-6_2021-07-28_23-42-59.msd\n",
      "(1892, 128, 128, 2)\n",
      "Testing round time: 0:00:43.738957\n",
      "\n",
      "Total testing time:   0:05:47.009070\n"
     ]
    }
   ],
   "source": [
    "# Main loop, test all models and save test results\n",
    "\n",
    "time_sequence_start = datetime.datetime.now()\n",
    "\n",
    "print(\"Using timestemp to find files: {}\".format(train_timestamp))\n",
    "print(\"Saving test predictions in:    {}\".format(test_predictions_fullpath))\n",
    "\n",
    "test_best_metrics    = dict()\n",
    "test_fuzzy_metrics   = dict()\n",
    "test_aurocs          = np.zeros(num_models)\n",
    "test_best_thresholds = np.zeros(num_models)\n",
    "test_prediction_time = np.zeros(num_models)\n",
    "\n",
    "for i in range(num_models):\n",
    "    time_round_start = datetime.datetime.now()\n",
    "    \n",
    "    model = UNet(128,2).eval()\n",
    "    \n",
    "    print(\"Testing model: {}\".format(model_file_list[i]))\n",
    "    model.load_state_dict(torch.load(model_file_list[i]))\n",
    "    test_prediction = model(test_ultrasound_array)\n",
    "    test_prediction = torch.nn.Softmax(dim=1)(test_prediction)\n",
    "    test_prediction = test_prediction.permute(0,2,3,1)\n",
    "    test_prediction = np.array(test_prediction.detach().numpy())\n",
    "    print(np.shape(test_prediction))\n",
    "    \n",
    "    test_prediction_filename = train_timestamp + \"_prediction_test.npy\"\n",
    "    test_prediction_fullname = os.path.join(test_predictions_fullpath, test_prediction_filename)\n",
    "    np.save(test_prediction_fullname, test_prediction)\n",
    "    \n",
    "    # Test results\n",
    "    \n",
    "    test_metrics_dicts, test_best_threshold_index, test_area = evaluation_metrics.compute_roc(\n",
    "        roc_thresholds, test_prediction, test_segmentation_array, acceptable_margin_mm, mm_per_pixel)\n",
    "    \n",
    "    test_fuzzy_metrics[i] = evaluation_metrics.compute_evaluation_metrics(\n",
    "        test_prediction, test_segmentation_array, acceptable_margin_mm, mm_per_pixel)\n",
    "    \n",
    "    test_best_metrics[i]    = test_metrics_dicts[test_best_threshold_index]\n",
    "    test_aurocs[i]          = test_area\n",
    "    test_best_thresholds[i] = roc_thresholds[test_best_threshold_index]\n",
    "    \n",
    "    print(\"Testing round time: {}\".format(datetime.datetime.now() - time_round_start))\n",
    "\n",
    "\n",
    "time_sequence_stop = datetime.datetime.now()\n",
    "\n",
    "print(\"\\nTotal testing time:   {}\".format(time_sequence_stop - time_sequence_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test AUROC</th>\n",
       "      <th>Test best thresh</th>\n",
       "      <th>Test best TP</th>\n",
       "      <th>Test best FP</th>\n",
       "      <th>Test best recall</th>\n",
       "      <th>Test best precis</th>\n",
       "      <th>Test fuzzy recall</th>\n",
       "      <th>Test fuzzy precis</th>\n",
       "      <th>Test fuzzy Fscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.880390</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.929612</td>\n",
       "      <td>0.232987</td>\n",
       "      <td>0.929612</td>\n",
       "      <td>0.010971</td>\n",
       "      <td>0.058482</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.021939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.873368</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.879428</td>\n",
       "      <td>0.187280</td>\n",
       "      <td>0.879428</td>\n",
       "      <td>0.012885</td>\n",
       "      <td>0.052202</td>\n",
       "      <td>0.012385</td>\n",
       "      <td>0.020020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.890357</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.908031</td>\n",
       "      <td>0.190649</td>\n",
       "      <td>0.908031</td>\n",
       "      <td>0.013067</td>\n",
       "      <td>0.064450</td>\n",
       "      <td>0.013880</td>\n",
       "      <td>0.022841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.875339</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.922251</td>\n",
       "      <td>0.233923</td>\n",
       "      <td>0.922251</td>\n",
       "      <td>0.010842</td>\n",
       "      <td>0.052323</td>\n",
       "      <td>0.013162</td>\n",
       "      <td>0.021033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.883948</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.942666</td>\n",
       "      <td>0.245236</td>\n",
       "      <td>0.942666</td>\n",
       "      <td>0.010574</td>\n",
       "      <td>0.056003</td>\n",
       "      <td>0.014845</td>\n",
       "      <td>0.023468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.846527</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.850429</td>\n",
       "      <td>0.228931</td>\n",
       "      <td>0.850429</td>\n",
       "      <td>0.010220</td>\n",
       "      <td>0.041621</td>\n",
       "      <td>0.011437</td>\n",
       "      <td>0.017943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.860039</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.901942</td>\n",
       "      <td>0.231736</td>\n",
       "      <td>0.901942</td>\n",
       "      <td>0.010704</td>\n",
       "      <td>0.053601</td>\n",
       "      <td>0.006347</td>\n",
       "      <td>0.011350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.862573</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.881307</td>\n",
       "      <td>0.209639</td>\n",
       "      <td>0.881307</td>\n",
       "      <td>0.011551</td>\n",
       "      <td>0.035447</td>\n",
       "      <td>0.012941</td>\n",
       "      <td>0.018960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Test AUROC  Test best thresh  Test best TP  Test best FP  Test best recall  \\\n",
       "0    0.880390             0.010      0.929612      0.232987          0.929612   \n",
       "1    0.873368             0.020      0.879428      0.187280          0.879428   \n",
       "2    0.890357             0.020      0.908031      0.190649          0.908031   \n",
       "3    0.875339             0.010      0.922251      0.233923          0.922251   \n",
       "4    0.883948             0.010      0.942666      0.245236          0.942666   \n",
       "5    0.846527             0.006      0.850429      0.228931          0.850429   \n",
       "6    0.860039             0.010      0.901942      0.231736          0.901942   \n",
       "7    0.862573             0.008      0.881307      0.209639          0.881307   \n",
       "\n",
       "   Test best precis  Test fuzzy recall  Test fuzzy precis  Test fuzzy Fscore  \n",
       "0          0.010971           0.058482           0.013502           0.021939  \n",
       "1          0.012885           0.052202           0.012385           0.020020  \n",
       "2          0.013067           0.064450           0.013880           0.022841  \n",
       "3          0.010842           0.052323           0.013162           0.021033  \n",
       "4          0.010574           0.056003           0.014845           0.023468  \n",
       "5          0.010220           0.041621           0.011437           0.017943  \n",
       "6          0.010704           0.053601           0.006347           0.011350  \n",
       "7          0.011551           0.035447           0.012941           0.018960  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Averages\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test AUROC           0.871568\n",
       "Test best thresh     0.011750\n",
       "Test best TP         0.901958\n",
       "Test best FP         0.220048\n",
       "Test best recall     0.901958\n",
       "Test best precis     0.011352\n",
       "Test fuzzy recall    0.051766\n",
       "Test fuzzy precis    0.012312\n",
       "Test fuzzy Fscore    0.019694\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Arrange results in tables\n",
    "\n",
    "metric_labels = [\n",
    "    \"AUROC\",\n",
    "    \"best thresh\",\n",
    "    \"best TP\",\n",
    "    \"best FP\",\n",
    "    \"best recall\",\n",
    "    \"best precis\",\n",
    "    \"fuzzy recall\",\n",
    "    \"fuzzy precis\",\n",
    "    \"fuzzy Fscore\"\n",
    "]\n",
    "\n",
    "results_labels = []\n",
    "\n",
    "for label in metric_labels:\n",
    "    results_labels.append(\"Test \" + label)\n",
    "\n",
    "    \n",
    "results_df = pd.DataFrame(columns = results_labels)\n",
    "\n",
    "for i in range(num_rounds):\n",
    "    results_df.loc[i] = [\n",
    "        test_aurocs[i],\n",
    "        test_best_thresholds[i],\n",
    "        test_best_metrics[i][evaluation_metrics.TRUE_POSITIVE_RATE],\n",
    "        test_best_metrics[i][evaluation_metrics.FALSE_POSITIVE_RATE],\n",
    "        test_best_metrics[i][evaluation_metrics.RECALL],\n",
    "        test_best_metrics[i][evaluation_metrics.PRECISION],\n",
    "        test_fuzzy_metrics[i][evaluation_metrics.RECALL],\n",
    "        test_fuzzy_metrics[i][evaluation_metrics.PRECISION],\n",
    "        test_fuzzy_metrics[i][evaluation_metrics.FSCORE],\n",
    "    ]\n",
    "\n",
    "display(results_df)\n",
    "\n",
    "print(\"\\nAverages\")\n",
    "\n",
    "results_means_df = results_df.mean()\n",
    "display(results_means_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results table\n",
    "\n",
    "csv_filename = this_notebook_name + \"_\" + train_timestamp + \".csv\"\n",
    "csv_fullname = os.path.join(results_save_fullpath, csv_filename)\n",
    "results_df.to_csv(csv_fullname)\n",
    "\n",
    "print(\"Results saved to: {}\".format(csv_fullname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(float(str(sf_time)[5:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# GET SINGLE-FRAME TIMINGS\n",
    "\n",
    "num_test = test_ultrasound_array.shape[0]\n",
    "num_show = 4\n",
    "\n",
    "indices = [i for i in range(num_test)]\n",
    "sample_indices = sample(indices, num_show)\n",
    "\n",
    "model = load_model(model_file_list[1], custom_objects={'loss': wcce, 'dice_coef': dice_coef})\n",
    "# test_prediction_singleframe = model.predict(test_ultrasound_array[sample_indices[0]][np.newaxis, :, :, :])\n",
    "\n",
    "fig = plt.figure(figsize=(18, num_show*5))\n",
    "for i in range(num_show):\n",
    "    sf_start = datetime.datetime.now()\n",
    "    test_prediction_singleframe = model.predict(test_ultrasound_array[sample_indices[i]][np.newaxis, :, :, :])\n",
    "    test_prediction_singleframe = np.array(test_prediction_singleframe)\n",
    "    b = np.sum(test_prediction_singleframe)\n",
    "    sf_end = datetime.datetime.now()\n",
    "    \n",
    "    sf_time = sf_end - sf_start\n",
    "#     print(sf_time.dtype)\n",
    "#     print(np.shape(test_prediction_singleframe))\n",
    "    \n",
    "    a0 = fig.add_subplot(num_show,3,i*3+1)\n",
    "    img0 = a0.imshow(test_ultrasound_array[sample_indices[i], :, :, 0].astype(np.float32))\n",
    "    a0.set_title(\"Ultrasound #{}\".format(sample_indices[i]), fontsize=18)\n",
    "    a1 = fig.add_subplot(num_show,3,i*3+2)\n",
    "    img1 = a1.imshow(test_segmentation_array[sample_indices[i], :, :, 0], vmin=0.0, vmax=1.0)\n",
    "    a1.set_title(\"Segmentation #{}\".format(sample_indices[i]), fontsize=18)\n",
    "    c = fig.colorbar(img1, fraction=0.046, pad=0.04)\n",
    "    a2 = fig.add_subplot(num_show,3,i*3+3)\n",
    "    img2 = a2.imshow(test_prediction_singleframe[0, :, :, 1], vmin=0.0, vmax=1.0)\n",
    "    a2.set_title(\"Prediction #{} (\".format(sample_indices[i]) + str(sf_time)[5:] + \" seconds)\", fontsize=18)\n",
    "    c = fig.colorbar(img2, fraction=0.046, pad=0.04)\n",
    "    \n",
    "fig.suptitle(\"U-Net Single-frame Prediction Timings on Saggital Spine Data\\n\", fontsize=30)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET MULTI-FRAME TIMINGS\n",
    "\n",
    "tabl = PrettyTable()\n",
    "\n",
    "\n",
    "metric_labels = [\n",
    "    \"Frames\",\n",
    "    \"# Samples\",\n",
    "    \"First PT (s)\",\n",
    "    \"Avg PT (s)\",\n",
    "    \"Min PT (s)\",\n",
    "    \"Max PT (s)\",\n",
    "    \"Std. Dev. (s)\",\n",
    "]\n",
    "\n",
    "tabl.field_names = metric_labels\n",
    "results_df = pd.DataFrame(columns = metric_labels)\n",
    "\n",
    "# Main Loop\n",
    "\n",
    "frames = [1, 10, 20, 30, 40, 50]\n",
    "num_test = test_ultrasound_array.shape[0]\n",
    "\n",
    "for l in range(len(frames)):\n",
    "\n",
    "    indices = [i for i in range(num_test - frames[l])]\n",
    "\n",
    "    model = load_model(model_file_list[1], custom_objects={'loss': wcce, 'dice_coef': dice_coef})\n",
    "    mf_times = []\n",
    "\n",
    "    for i in indices:\n",
    "        mf_start = datetime.datetime.now()\n",
    "        test_prediction_singleframe = model.predict(test_ultrasound_array[i:i + frames[l], :, :, :])\n",
    "        test_prediction_singleframe = np.array(test_prediction_singleframe)\n",
    "        b = np.sum(test_prediction_singleframe)\n",
    "        mf_end = datetime.datetime.now()\n",
    "\n",
    "        mf_time = mf_end - mf_start\n",
    "        mf_time_seconds = float(str(mf_time)[5:])\n",
    "        mf_times.append(mf_time_seconds)\n",
    "    \n",
    "#     tabl.add_row([\n",
    "#         frames[l],\n",
    "#         num_test - frames[l],\n",
    "#         mf_times[0],\n",
    "#         np.mean(mf_times),\n",
    "#         np.min(mf_times),\n",
    "#         np.max(mf_times),\n",
    "#         np.std(mf_times)\n",
    "#     ])\n",
    "    \n",
    "    results_df.loc[l] = [\n",
    "        frames[l],\n",
    "        num_test - frames[l],\n",
    "        mf_times[0],\n",
    "        int((np.mean(mf_times)*100000)) / 100000,\n",
    "        np.min(mf_times),\n",
    "        np.max(mf_times),\n",
    "        int((np.std(mf_times)*100000)) / 100000\n",
    "    ]\n",
    "\n",
    "# print(tabl)\n",
    "print(\"Multi-Frame \\\"Sliding Window\\\" Prediction Times on Saggital Spine Data\")\n",
    "print(\"PT = Prediction Time\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET MULTI-FRAME TIMINGS WITH WARM-UP\n",
    "\n",
    "tabl = PrettyTable()\n",
    "\n",
    "\n",
    "metric_labels = [\n",
    "    \"Frames\",\n",
    "    \"# Samples\",\n",
    "    \"First PT (s)\",\n",
    "    \"Avg PT (s)\",\n",
    "    \"Min PT (s)\",\n",
    "    \"Max PT (s)\",\n",
    "    \"Std. Dev. (s)\",\n",
    "]\n",
    "\n",
    "tabl.field_names = metric_labels\n",
    "results_df = pd.DataFrame(columns = metric_labels)\n",
    "\n",
    "# Main Loop\n",
    "\n",
    "frames = [1, 10, 20, 30, 40, 50]\n",
    "num_test = test_ultrasound_array.shape[0]\n",
    "\n",
    "for l in range(len(frames)):\n",
    "\n",
    "    indices = [i for i in range(num_test - frames[l])]\n",
    "\n",
    "    model = load_model(model_file_list[1], custom_objects={'loss': wcce, 'dice_coef': dice_coef})\n",
    "    test_prediction_singleframe = model.predict(test_ultrasound_array[0:0 + frames[l], :, :, :])\n",
    "    mf_times = []\n",
    "\n",
    "    for i in indices:\n",
    "        mf_start = datetime.datetime.now()\n",
    "        test_prediction_singleframe = model.predict(test_ultrasound_array[i:i + frames[l], :, :, :])\n",
    "        test_prediction_singleframe = np.array(test_prediction_singleframe)\n",
    "        b = np.sum(test_prediction_singleframe)\n",
    "        mf_end = datetime.datetime.now()\n",
    "\n",
    "        mf_time = mf_end - mf_start\n",
    "        mf_time_seconds = float(str(mf_time)[5:])\n",
    "        mf_times.append(mf_time_seconds)\n",
    "    \n",
    "#     tabl.add_row([\n",
    "#         frames[l],\n",
    "#         num_test - frames[l],\n",
    "#         mf_times[0],\n",
    "#         np.mean(mf_times),\n",
    "#         np.min(mf_times),\n",
    "#         np.max(mf_times),\n",
    "#         np.std(mf_times)\n",
    "#     ])\n",
    "    \n",
    "    results_df.loc[l] = [\n",
    "        frames[l],\n",
    "        num_test - frames[l],\n",
    "        mf_times[0],\n",
    "        int((np.mean(mf_times)*100000)) / 100000,\n",
    "        np.min(mf_times),\n",
    "        np.max(mf_times),\n",
    "        int((np.std(mf_times)*100000)) / 100000\n",
    "    ]\n",
    "\n",
    "# print(tabl)\n",
    "print(\"Multi-Frame \\\"Sliding Window\\\" Prediction Times with \\\"warm-up\\\" on Saggital Spine Data\")\n",
    "print(\"PT = Prediction Time\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save notebook so all output is archived by the next cell\n",
    "\n",
    "from IPython.display import Javascript\n",
    "script = '''\n",
    "require([\"base/js/namespace\"],function(Jupyter) {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "});\n",
    "'''\n",
    "Javascript(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export HTML copy of this notebook\n",
    "\n",
    "notebook_file_name = this_notebook_name + \"_\" + train_timestamp + \".html\"\n",
    "notebook_fullname = os.path.join(notebooks_save_fullpath, notebook_file_name)\n",
    "\n",
    "os.system(\"jupyter nbconvert --to html \" + this_notebook_name + \" --output \" + notebook_fullname)\n",
    "print(\"Notebook saved to: {}\".format(notebook_fullname))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
