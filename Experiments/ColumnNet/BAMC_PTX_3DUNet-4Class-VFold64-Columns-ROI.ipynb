{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e41c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (\n",
    "    AddChanneld,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    RandFlipd,\n",
    "    RandSpatialCropd,\n",
    "    RandZoomd,\n",
    "    Resized,\n",
    "    ScaleIntensityRanged,\n",
    "    SpatialCrop,\n",
    "    SpatialCropd,\n",
    "    ToTensord,\n",
    ")\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import itk\n",
    "\n",
    "import site\n",
    "site.addsitedir('../../ARGUS')\n",
    "from ARGUSUtils_Transforms import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ef91fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Images = 90\n",
      "# of Labels = 90\n"
     ]
    }
   ],
   "source": [
    "img1_dir = \"../../Data/VFoldData/ColumnData/\"\n",
    "\n",
    "all_images = sorted(glob(os.path.join(img1_dir, '*Class[NS]*[0123456789].mha')))\n",
    "all_labels = sorted(glob(os.path.join(img1_dir, '*Class[NS]*.roi-overlay.mha')))\n",
    "\n",
    "print(\"# of Images =\", len(all_images))\n",
    "print(\"# of Labels =\", len(all_labels))\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "num_columns = 16\n",
    "num_slices = 48\n",
    "\n",
    "num_workers_tr = 8\n",
    "batch_size_tr = 8\n",
    "num_workers_vl = 8\n",
    "batch_size_vl = 4\n",
    "\n",
    "model_filename_base = \"BAMC_PTX_3DUNet-4Class.best_model.vfold64-Columns-ROI\"\n",
    "\n",
    "num_images = len(all_images)\n",
    "\n",
    "num_folds = 10\n",
    "\n",
    "ns_prefix = ['025ns','026ns','027ns','035ns','048ns','055ns','117ns',\n",
    "             '135ns','193ns','210ns','215ns','218ns','219ns','221ns','247ns']\n",
    "s_prefix = ['004s','019s','030s','034s','037s','043s','065s','081s',\n",
    "            '206s','208s','211s','212s','224s','228s','236s','237s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df21ce2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 14 6\n",
      "74 6 10\n",
      "72 10 8\n",
      "73 8 9\n",
      "73 9 8\n",
      "74 8 8\n",
      "75 8 7\n",
      "76 7 7\n",
      "70 7 13\n",
      "63 13 14\n"
     ]
    }
   ],
   "source": [
    "fold_prefix_list = []\n",
    "ns_count = 0\n",
    "s_count = 0\n",
    "for i in range(num_folds):\n",
    "    if i%2 == 0:\n",
    "        num_ns = 2\n",
    "        num_s = 1\n",
    "        if i > num_folds-3:\n",
    "            num_s = 2\n",
    "    else:\n",
    "        num_ns = 1\n",
    "        num_s = 2\n",
    "    f = []\n",
    "    for ns in range(num_ns):\n",
    "        f.append([ns_prefix[ns_count+ns]])\n",
    "    ns_count += num_ns\n",
    "    for s in range(num_s):\n",
    "        f.append([s_prefix[s_count+s]])\n",
    "    s_count += num_s\n",
    "    fold_prefix_list.append(f)\n",
    "        \n",
    "train_files = []\n",
    "val_files = []\n",
    "test_files = []\n",
    "for i in range(num_folds):\n",
    "    tr_folds = []\n",
    "    for f in range(i,i+num_folds-2):\n",
    "        tr_folds.append(fold_prefix_list[f%num_folds])\n",
    "    tr_folds = list(np.concatenate(tr_folds).flat)\n",
    "    va_folds = list(np.concatenate(fold_prefix_list[(i+num_folds-2) % num_folds]).flat)\n",
    "    te_folds = list(np.concatenate(fold_prefix_list[(i+num_folds-1) % num_folds]).flat)\n",
    "    train_files.append(\n",
    "            [\n",
    "                {\"image\": img, \"label\": seg}\n",
    "                for img, seg in zip(\n",
    "                    [im for im in all_images if any(pref in im for pref in tr_folds)],\n",
    "                    [se for se in all_labels if any(pref in se for pref in tr_folds)])\n",
    "            ]\n",
    "        )\n",
    "    val_files.append(\n",
    "            [\n",
    "                {\"image\": img, \"label\": seg}\n",
    "                for img, seg in zip(\n",
    "                    [im for im in all_images if any(pref in im for pref in va_folds)],\n",
    "                    [se for se in all_labels if any(pref in se for pref in va_folds)])\n",
    "            ]\n",
    "        )\n",
    "    test_files.append(\n",
    "            [\n",
    "                {\"image\": img, \"label\": seg}\n",
    "                for img, seg in zip(\n",
    "                    [im for im in all_images if any(pref in im for pref in te_folds)],\n",
    "                    [se for se in all_labels if any(pref in se for pref in te_folds)])\n",
    "            ]\n",
    "        )\n",
    "    print(len(train_files[i]),len(val_files[i]),len(test_files[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9455d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        AddChanneld(keys=[\"image\", \"label\"]),\n",
    "        ScaleIntensityRanged(keys=[\"image\"],\n",
    "            a_min=0, a_max=255,\n",
    "            b_min=0.0, b_max=1.0),\n",
    "        ARGUS_RandSpatialCropSlicesd(\n",
    "            num_slices=num_slices,\n",
    "            axis=2,\n",
    "            keys=['image', 'label']),\n",
    "        ARGUS_RandSpatialCropSlicesd(\n",
    "            num_slices=num_columns,\n",
    "            axis=0,\n",
    "            require_labeled=True,\n",
    "            keys=['image', 'label']),\n",
    "        RandFlipd(prob=0.5, \n",
    "            spatial_axis=2,\n",
    "            keys=['image', 'label']),\n",
    "        RandFlipd(prob=0.5, \n",
    "            spatial_axis=0,\n",
    "            keys=['image', 'label']),\n",
    "        RandZoomd(prob=0.5, \n",
    "            min_zoom=1.0,\n",
    "            max_zoom=1.2,\n",
    "            keys=['image', 'label'],\n",
    "            keep_size=True,\n",
    "            mode=['trilinear', 'nearest']),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        AddChanneld(keys=[\"image\", \"label\"]),\n",
    "        ScaleIntensityRanged(keys=[\"image\"],\n",
    "            a_min=0, a_max=255,\n",
    "            b_min=0.0, b_max=1.0),\n",
    "        ARGUS_RandSpatialCropSlicesd(\n",
    "            num_slices=num_slices,\n",
    "            axis=2,\n",
    "            center_slice=30,\n",
    "            keys=['image', 'label']),\n",
    "        ARGUS_RandSpatialCropSlicesd(\n",
    "            num_slices=num_columns,\n",
    "            axis=0,\n",
    "            center_slice=num_columns//2,\n",
    "            keys=['image', 'label']),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "196d9550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image (0x55a6d78d1630)\n",
      "  RTTI typeinfo:   itk::Image<short, 3u>\n",
      "  Reference Count: 1\n",
      "  Modified Time: 442\n",
      "  Debug: Off\n",
      "  Object Name: \n",
      "  Observers: \n",
      "    none\n",
      "  Source: (none)\n",
      "  Source output name: (none)\n",
      "  Release Data: Off\n",
      "  Data Released: False\n",
      "  Global Release Data: Off\n",
      "  PipelineMTime: 246\n",
      "  UpdateMTime: 441\n",
      "  RealTimeStamp: 0 seconds \n",
      "  LargestPossibleRegion: \n",
      "    Dimension: 3\n",
      "    Index: [0, 0, 0]\n",
      "    Size: [93, 320, 61]\n",
      "  BufferedRegion: \n",
      "    Dimension: 3\n",
      "    Index: [0, 0, 0]\n",
      "    Size: [93, 320, 61]\n",
      "  RequestedRegion: \n",
      "    Dimension: 3\n",
      "    Index: [0, 0, 0]\n",
      "    Size: [93, 320, 61]\n",
      "  Spacing: [1, 1, 1]\n",
      "  Origin: [0, 0, 0]\n",
      "  Direction: \n",
      "1 0 0\n",
      "0 1 0\n",
      "0 0 1\n",
      "\n",
      "  IndexToPointMatrix: \n",
      "1 0 0\n",
      "0 1 0\n",
      "0 0 1\n",
      "\n",
      "  PointToIndexMatrix: \n",
      "1 0 0\n",
      "0 1 0\n",
      "0 0 1\n",
      "\n",
      "  Inverse Direction: \n",
      "1 0 0\n",
      "0 1 0\n",
      "0 0 1\n",
      "\n",
      "  PixelContainer: \n",
      "    ImportImageContainer (0x55a6d81e97b0)\n",
      "      RTTI typeinfo:   itk::ImportImageContainer<unsigned long, short>\n",
      "      Reference Count: 1\n",
      "      Modified Time: 438\n",
      "      Debug: Off\n",
      "      Object Name: \n",
      "      Observers: \n",
      "        none\n",
      "      Pointer: 0x7f989d6ef010\n",
      "      Container manages memory: true\n",
      "      Size: 1815360\n",
      "      Capacity: 1815360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itkForceDynamicLoading = itk.imread(train_files[0][0][\"image\"])\n",
    "print(itkForceDynamicLoading)\n",
    "arrForceDynamicLoading = itk.GetArrayFromImage(itkForceDynamicLoading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5693f621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████████████████| 70/70 [00:01<00:00, 57.34it/s]\n",
      "Loading dataset: 100%|██████████████████████| 74/74 [00:01<00:00, 60.35it/s]\n",
      "Loading dataset: 100%|██████████████████████| 72/72 [00:01<00:00, 64.37it/s]\n",
      "Loading dataset: 100%|██████████████████████| 73/73 [00:01<00:00, 60.50it/s]\n",
      "Loading dataset: 100%|██████████████████████| 73/73 [00:01<00:00, 63.22it/s]\n",
      "Loading dataset: 100%|██████████████████████| 74/74 [00:01<00:00, 66.81it/s]\n",
      "Loading dataset: 100%|██████████████████████| 75/75 [00:01<00:00, 66.48it/s]\n",
      "Loading dataset: 100%|██████████████████████| 76/76 [00:01<00:00, 56.59it/s]\n",
      "Loading dataset: 100%|██████████████████████| 70/70 [00:01<00:00, 63.59it/s]\n",
      "Loading dataset: 100%|██████████████████████| 63/63 [00:01<00:00, 56.87it/s]\n",
      "Loading dataset: 100%|█████████████████████| 14/14 [00:00<00:00, 125.28it/s]\n",
      "Loading dataset: 100%|███████████████████████| 6/6 [00:00<00:00, 100.38it/s]\n",
      "Loading dataset: 100%|████████████████████| 10/10 [00:00<00:00, 1139.38it/s]\n",
      "Loading dataset: 100%|███████████████████████| 8/8 [00:00<00:00, 276.66it/s]\n",
      "Loading dataset: 100%|███████████████████████| 9/9 [00:00<00:00, 195.50it/s]\n",
      "Loading dataset: 100%|███████████████████████| 8/8 [00:00<00:00, 720.73it/s]\n",
      "Loading dataset: 100%|███████████████████████| 8/8 [00:00<00:00, 148.96it/s]\n",
      "Loading dataset: 100%|███████████████████████| 7/7 [00:00<00:00, 302.87it/s]\n",
      "Loading dataset: 100%|███████████████████████| 7/7 [00:00<00:00, 577.48it/s]\n",
      "Loading dataset: 100%|██████████████████████| 13/13 [00:00<00:00, 72.70it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = [CacheDataset(data=train_files[i], transform=train_transforms,cache_rate=1.0, num_workers=num_workers_tr)\n",
    "            for i in range(num_folds)]\n",
    "train_loader = [DataLoader(train_ds[i], batch_size=batch_size_tr, shuffle=True, num_workers=num_workers_tr) \n",
    "                for i in range(num_folds)]\n",
    "\n",
    "val_ds = [CacheDataset(data=val_files[i], transform=val_transforms, cache_rate=1.0, num_workers=num_workers_vl)\n",
    "          for i in range(num_folds)]\n",
    "val_loader = [DataLoader(val_ds[i], batch_size=batch_size_vl, num_workers=num_workers_vl)\n",
    "              for i in range(num_folds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc3f1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23757880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vfold_train(vfold_num, train_loader, val_loader):\n",
    "    model = UNet(\n",
    "        dimensions=3,\n",
    "        in_channels=1,\n",
    "        out_channels=num_classes,\n",
    "        channels=(16, 32, 64, 128),\n",
    "        strides=(2, 2, 2),\n",
    "        num_res_units=2,\n",
    "        norm=Norm.BATCH,\n",
    "    ).to(device)\n",
    "    loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "    dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "\n",
    "\n",
    "    max_epochs = 1000\n",
    "    val_interval = 2\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    epoch_loss_values = []\n",
    "    metric_values = []\n",
    "    post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=True, num_classes=num_classes)])\n",
    "    post_label = Compose([EnsureType(), AsDiscrete(to_onehot=True, num_classes=num_classes)])\n",
    "\n",
    "    root_dir = \".\"\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"{vfold_num}: epoch {epoch + 1}/{max_epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs, labels = (\n",
    "                batch_data[\"image\"].to(device),\n",
    "                batch_data[\"label\"].to(device),\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            print(\n",
    "                f\"{step}/{len(train_ds) // train_loader.batch_size}, \"\n",
    "                f\"train_loss: {loss.item():.4f}\")\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"{vfold_num} epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_data in val_loader:\n",
    "                    val_inputs, val_labels = (\n",
    "                        val_data[\"image\"].to(device),\n",
    "                        val_data[\"label\"].to(device),\n",
    "                    )\n",
    "                    roi_size = (num_columns, 320, num_slices)\n",
    "                    sw_batch_size = batch_size_vl\n",
    "                    val_outputs = sliding_window_inference(\n",
    "                        val_inputs, roi_size, sw_batch_size, model)\n",
    "                    val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                    val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                    # compute metric for current iteration\n",
    "                    dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "                # aggregate the final mean dice result\n",
    "                metric = dice_metric.aggregate().item()\n",
    "                # reset the status for next validation round\n",
    "                dice_metric.reset()\n",
    "\n",
    "                metric_values.append(metric)\n",
    "                if metric >= best_metric:\n",
    "                    best_metric = metric\n",
    "                    best_metric_epoch = epoch + 1\n",
    "                    torch.save(model.state_dict(), os.path.join(\n",
    "                        root_dir, model_filename_base+'_'+str(vfold_num)+'.pth'))\n",
    "                    print(\"saved new best metric model\")\n",
    "                print(\n",
    "                    f\"current epoch: {epoch + 1} current mean dice: {metric:.8f}\"\n",
    "                    f\"\\nbest mean dice: {best_metric:.8f} \"\n",
    "                    f\"at epoch: {best_metric_epoch}\"\n",
    "                )\n",
    "\n",
    "    np.save(model_filename_base+\"_loss_\"+str(vfold_num)+\".npy\", epoch_loss_values)\n",
    "    np.save(model_filename_base+\"_val_dice_\"+str(vfold_num)+\".npy\", metric_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0371ecd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "0: epoch 1/1000\n",
      "1/1, train_loss: 0.8599\n",
      "2/1, train_loss: 0.8591\n",
      "3/1, train_loss: 0.8547\n",
      "4/1, train_loss: 0.8521\n",
      "5/1, train_loss: 0.8485\n",
      "6/1, train_loss: 0.8435\n",
      "7/1, train_loss: 0.8400\n",
      "8/1, train_loss: 0.8349\n",
      "9/1, train_loss: 0.8372\n",
      "0 epoch 1 average loss: 0.8477\n",
      "----------\n",
      "0: epoch 2/1000\n",
      "1/1, train_loss: 0.8327\n",
      "2/1, train_loss: 0.8285\n",
      "3/1, train_loss: 0.8252\n",
      "4/1, train_loss: 0.8236\n",
      "5/1, train_loss: 0.8219\n",
      "6/1, train_loss: 0.8315\n",
      "7/1, train_loss: 0.8210\n",
      "8/1, train_loss: 0.8116\n",
      "9/1, train_loss: 0.8193\n",
      "0 epoch 2 average loss: 0.8239\n",
      "saved new best metric model\n",
      "current epoch: 2 current mean dice: 0.09288500\n",
      "best mean dice: 0.09288500 at epoch: 2\n",
      "----------\n",
      "0: epoch 3/1000\n",
      "1/1, train_loss: 0.8279\n",
      "2/1, train_loss: 0.8086\n",
      "3/1, train_loss: 0.8177\n",
      "4/1, train_loss: 0.8186\n",
      "5/1, train_loss: 0.8063\n",
      "6/1, train_loss: 0.8096\n",
      "7/1, train_loss: 0.8139\n",
      "8/1, train_loss: 0.7948\n",
      "9/1, train_loss: 0.7994\n",
      "0 epoch 3 average loss: 0.8108\n",
      "----------\n",
      "0: epoch 4/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7f995db1e940>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/local/KHQ/stephen.aylward/anaconda3/lib/python3.8/logging/__init__.py\", line 223, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 2004356, 2004500, 2004574, 2004720, 2004866, 2004938, 2005094) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1957266/2402825631.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mvfold_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1957266/197204998.py\u001b[0m in \u001b[0;36mvfold_train\u001b[0;34m(vfold_num, train_loader, val_loader)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             inputs, labels = (\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 2004356, 2004500, 2004574, 2004720, 2004866, 2004938, 2005094) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "for i in range(0,num_folds):\n",
    "    vfold_train(i, train_loader[i], val_loader[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd8c719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
